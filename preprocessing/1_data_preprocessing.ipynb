{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da9b5beb6e9739b8"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-12T12:20:11.738261500Z",
     "start_time": "2024-08-12T12:20:11.662610300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Data Loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "133dd419518598bf"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def parse_log_to_dataframe(log_file_path):\n",
    "    \"\"\"\n",
    "    Parses a log file and converts it into a Pandas DataFrame with all data as strings.\n",
    "\n",
    "    Parameters:\n",
    "    log_file_path (str): Path to the log file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the parsed data, with all fields as strings.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    columns = []\n",
    "\n",
    "    # Open and read the log file\n",
    "    with open(log_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('#fields'):\n",
    "                columns = line.strip().split('\\x09')[1:]\n",
    "            elif not line.startswith('#'):\n",
    "                data.append(line.strip().split('\\x09'))\n",
    "\n",
    "    # Creating the DataFrame with all data as strings\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-12T12:20:11.759348700Z",
     "start_time": "2024-08-12T12:20:11.668154300Z"
    }
   },
   "id": "43a044786765e86c"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def load_zeek_logs(log_dir):\n",
    "    \"\"\"Load all Zeek logs from the directory.\"\"\"\n",
    "    logs = {}\n",
    "    for file in os.listdir(log_dir):\n",
    "        if file.endswith(\".log\"):\n",
    "            log_path = os.path.join(log_dir, file)\n",
    "            logs[file.split('.')[0]] = parse_log_to_dataframe(log_path)\n",
    "    return logs\n",
    "\n",
    "log_dir = '../data/zeek'\n",
    "logs = load_zeek_logs(log_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-12T12:20:11.761843700Z",
     "start_time": "2024-08-12T12:20:11.680145500Z"
    }
   },
   "id": "9267572b1c511464"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13300247dae92af6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle timestamps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e59cc67b22dd9d32"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tigro\\AppData\\Local\\Temp\\ipykernel_13480\\637344723.py:1: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  logs['conn']['ts'] = pd.to_datetime(logs['conn']['ts'], unit='s')\n",
      "C:\\Users\\tigro\\AppData\\Local\\Temp\\ipykernel_13480\\637344723.py:2: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  logs['dns']['ts'] = pd.to_datetime(logs['dns']['ts'], unit='s')\n",
      "C:\\Users\\tigro\\AppData\\Local\\Temp\\ipykernel_13480\\637344723.py:3: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  logs['ssl']['ts'] = pd.to_datetime(logs['ssl']['ts'], unit='s')\n",
      "C:\\Users\\tigro\\AppData\\Local\\Temp\\ipykernel_13480\\637344723.py:4: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  logs['files']['ts'] = pd.to_datetime(logs['files']['ts'], unit='s')\n",
      "C:\\Users\\tigro\\AppData\\Local\\Temp\\ipykernel_13480\\637344723.py:5: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  logs['http']['ts'] = pd.to_datetime(logs['http']['ts'], unit='s')\n",
      "C:\\Users\\tigro\\AppData\\Local\\Temp\\ipykernel_13480\\637344723.py:6: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  logs['x509']['ts'] = pd.to_datetime(logs['x509']['ts'], unit='s')\n",
      "C:\\Users\\tigro\\AppData\\Local\\Temp\\ipykernel_13480\\637344723.py:7: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  logs['x509']['certificate.not_valid_before'] = pd.to_datetime(logs['x509']['certificate.not_valid_before'], unit='s')\n",
      "C:\\Users\\tigro\\AppData\\Local\\Temp\\ipykernel_13480\\637344723.py:8: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  logs['x509']['certificate.not_valid_after'] = pd.to_datetime(logs['x509']['certificate.not_valid_after'], unit='s')\n"
     ]
    }
   ],
   "source": [
    "logs['conn']['ts'] = pd.to_datetime(logs['conn']['ts'], unit='s')\n",
    "logs['dns']['ts'] = pd.to_datetime(logs['dns']['ts'], unit='s')\n",
    "logs['ssl']['ts'] = pd.to_datetime(logs['ssl']['ts'], unit='s')\n",
    "logs['files']['ts'] = pd.to_datetime(logs['files']['ts'], unit='s')\n",
    "logs['http']['ts'] = pd.to_datetime(logs['http']['ts'], unit='s')\n",
    "logs['x509']['ts'] = pd.to_datetime(logs['x509']['ts'], unit='s')\n",
    "logs['x509']['certificate.not_valid_before'] = pd.to_datetime(logs['x509']['certificate.not_valid_before'], unit='s')\n",
    "logs['x509']['certificate.not_valid_after'] = pd.to_datetime(logs['x509']['certificate.not_valid_after'], unit='s')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-12T12:20:11.762842900Z",
     "start_time": "2024-08-12T12:20:11.701321200Z"
    }
   },
   "id": "bd54b42372314420"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove redundant columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29850fef048b7e9f"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 4 columns from conn dataframe\n",
      "Removed 16 columns from dns dataframe\n",
      "Removed 6 columns from ssl dataframe\n",
      "Removed 11 columns from files dataframe\n",
      "Removed 15 columns from http dataframe\n",
      "Removed 9 columns from x509 dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tigro\\AppData\\Local\\Temp\\ipykernel_13480\\2886846793.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(placeholder, np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def remove_high_missing_and_constant_columns(df, df_name, threshold=90, placeholder='-'):\n",
    "    \"\"\"\n",
    "    Remove columns from the DataFrame where more than the specified percentage of values are missing\n",
    "    or contain a specified placeholder value, and also remove columns where all values are the same.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame from which columns will be removed.\n",
    "    - threshold (float): The percentage threshold to decide which columns to remove (default is 90%).\n",
    "    - placeholder (str): The placeholder value to be treated as missing (default is '-').\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned DataFrame with columns removed.\n",
    "    \"\"\"\n",
    "    # Replace placeholder values with NaN\n",
    "    df.replace(placeholder, np.nan, inplace=True)\n",
    "    \n",
    "    # Calculate percentage of missing values per column\n",
    "    missing_percentage = df.isna().mean() * 100\n",
    "    \n",
    "    # Identify columns where missing percentage is greater than the threshold\n",
    "    columns_to_drop_due_to_missing = missing_percentage[missing_percentage >= threshold].index\n",
    "    \n",
    "    # Identify columns where all values are the same\n",
    "    columns_to_drop_due_to_constant = df.columns[df.nunique() == 1]\n",
    "    \n",
    "    # Combine both criteria\n",
    "    columns_to_drop = columns_to_drop_due_to_missing.union(columns_to_drop_due_to_constant)\n",
    "    \n",
    "    print(f'Removed {len(columns_to_drop)} columns from {df_name} dataframe')\n",
    "    \n",
    "    # Drop the identified columns\n",
    "    cleaned_df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "logs['conn'] = remove_high_missing_and_constant_columns(logs['conn'], 'conn')\n",
    "logs['dns'] = remove_high_missing_and_constant_columns(logs['dns'], 'dns')\n",
    "logs['ssl'] = remove_high_missing_and_constant_columns(logs['ssl'], 'ssl')\n",
    "logs['files'] = remove_high_missing_and_constant_columns(logs['files'], 'files')\n",
    "logs['http'] = remove_high_missing_and_constant_columns(logs['http'], 'http')\n",
    "logs['x509'] = remove_high_missing_and_constant_columns(logs['x509'], 'x509')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-12T12:20:11.763841500Z",
     "start_time": "2024-08-12T12:20:11.717124500Z"
    }
   },
   "id": "c76815218c9199a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Data Saving"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9afe1c37de8fcaaa"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "logs['conn'].to_csv('../data/zeek_preprocessed/conn.csv', index=False)\n",
    "logs['dns'].to_csv('../data/zeek_preprocessed/dns.csv', index=False)\n",
    "logs['ssl'].to_csv('../data/zeek_preprocessed/ssl.csv', index=False)\n",
    "logs['files'].to_csv('../data/zeek_preprocessed/files.csv', index=False)\n",
    "logs['http'].to_csv('../data/zeek_preprocessed/http.csv', index=False)\n",
    "logs['x509'].to_csv('../data/zeek_preprocessed/x509.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-12T12:20:11.802553600Z",
     "start_time": "2024-08-12T12:20:11.751965400Z"
    }
   },
   "id": "ad43442edeebaa21"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
